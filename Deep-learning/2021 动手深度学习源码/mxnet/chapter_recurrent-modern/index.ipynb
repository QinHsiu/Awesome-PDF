{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 现代反复神经网络\n",
    ":label:`chap_modern_rnn`\n",
    "\n",
    "我们介绍了 RNN 的基础知识，它可以更好地处理序列数据。为了演示，我们在文本数据上实施了基于 RNN 的语言模型。但是，当从业人员现在面临着各种序列学习问题时，这些技术可能不足以使用他们。\n",
    "\n",
    "例如，实践中一个值得注意的问题是 RNN 的数量不稳定性。尽管我们已经应用了梯度剪切等实现技巧，但通过更复杂的序列模型设计，这个问题可以进一步缓解。具体来说，门控 RNN 在实践中更常见。我们将首先引入两个这样广泛使用的网络，即 * 门控循环单元 * (GRU) 和 * 长短期内存 * (LSTM)。此外，我们将使用迄今为止讨论的单个无向隐藏层来扩展 RNN 架构。我们将描述具有多个隐藏层的深层架构，并讨论双向设计与前向和向后重复计算。现代经常性网络经常采用这种扩张。在解释这些 RNN 变体时，我们将继续考虑 :numref:`chap_rnn` 中引入的语言建模问题。\n",
    "\n",
    "事实上，语言建模只能揭示序列学习能够实现的一小部分。在自动语音识别、文本转语音和机器翻译等各种序列学习问题中，输入和输出都是任意长度的序列。为了解释如何适应这种类型的数据，我们将以机器翻译为例，并介绍基于 rNN 和束搜索的编码器解码器架构，以便生成序列。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [gru](gru.ipynb)\n",
    " - [lstm](lstm.ipynb)\n",
    " - [deep-rnn](deep-rnn.ipynb)\n",
    " - [bi-rnn](bi-rnn.ipynb)\n",
    " - [machine-translation-and-dataset](machine-translation-and-dataset.ipynb)\n",
    " - [encoder-decoder](encoder-decoder.ipynb)\n",
    " - [seq2seq](seq2seq.ipynb)\n",
    " - [beam-search](beam-search.ipynb)\n",
    ":end_tab:\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}